{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello","text":"<p>I'm Seth and this is my digital garden and blog. My intention with this digital garden is to have a place to put information I learn and to build and share knowledge. Unlike blog posts, the entries in this learning garden won't be polished pieces of writing. In fact, if I'm successful at using my garden, there will always be incomplete entries because it will always be a work in progress. Given how blogs can often be oriented towards marketing and promotion, the temptation I'll try to avoid is worrying about how well something reads to others. In the garden, the primary audience is myself. However, posting it online will serve to share that information (and if I'm successful, knowledge) with others, as well as give me some accountability and connection in my learning journey.</p> <p>Some topics I may cultivate in my garden include: degrowth, post-capitalism, the solidarity economy, permaculture, and technology.</p> <p>My blog is currently a dozen technology-related articles from 2018-2019 when I was considering a career path in data science, so pretty dated stuff, but I figured I'd leave it as is. If I have future writing that feels more blog-like than garden-like then I'll stick it in the blog section.</p>"},{"location":"Degrowth/Definitions%20of%20degrowth/","title":"Definitions of degrowth","text":"<p>Jason Hickel\u2019s Degrowth and MMT: A thought experiment blog post has this succinct definition of degrowth:</p> <p>Degrowth has two parts: an ecology part and a social justice part. It seeks to (a) reduce excess resource and energy use (specifically in high-income nations) in order to bring the economy back into balance with the living world, and (b) to do so while at the same time reducing inequality and improving people\u2019s access to the things they need to live long, healthy, flourishing lives.</p>"},{"location":"Degrowth/Quotes/","title":"Quotes","text":"<p>From Jason Hickel\u2019s blog post, A response to Pollin and Chomsky: We need a Green New Deal without growth:</p> <p>Growth is ultimately a kind of propaganda term; it takes processes of extraction, commodification and elite accumulation, which are quite often destructive to human communities and to ecology, and sells them as natural, good and common-sense (who could possibly be against growth?). The language of growth is the bedrock of capitalism\u2019s cultural hegemony.</p>"},{"location":"Degrowth/Terminology/","title":"Terminology","text":"<p>I\u2019ve wondered about the difference between the terms \u201cpost-growth\u201d and \u201cdegrowth\u201d and found this article: Post-growth, degrowth, the doughnut and circular economy: a short guide (originally posted at planetamateur.com). Here are some of my takeaways mixed with my own thoughts:</p> <ul> <li>The circular economy contrasts with the linear economy, which is a one-way path that extracts resources, which are then used and thrown away. The circular economy tries to close loops (William McDonough\u2019s term \u201ccradle to cradle\u201d comes to mind). This can be done by making waste into resource inputs, both at the larger scale of manufacturing (I recall an actual business that uses waste coffee grounds to grow edible mushrooms), to the smaller scale of things like repair cafes.</li> <li>The doughnut or doughnut economy is Kate Raworth\u2019s term, illustrated below, which ties together meeting human needs across a dozen categories (avoiding shortfalls) while staying within nine types of ecological limits (avoiding overshoot). </li> <li>The article describes post-growth as an \u201capproach without a toolkit.\u201d Post-growth is the simple (and obvious) recognition that we can\u2019t endlessly expand production and consumption on a finite planet. As I\u2019ve been reading in more detail in Kohei Saito\u2019s  Slow Down, absolute decoupling (in which the economy grows while material consumption does not) is a nice concept to dream about, but is fantasy. Post-growth moves beyond seeking green technologies as the solution to our environmental problems to calling into question growth itself. (Green technologies, while certainly useful and important, will never be a sufficient solution to our environmental problems as long as we have endless growth.) I also think of post-growth as somewhat synonymous with the term \u201csteady-state economy.\u201d Both of these terms, to me, describe a future state that we intend to reach, but they don\u2019t describe how to get there.</li> <li>In contrast, degrowth is the \u201cagenda of action\u201d that can get us to the targets that the doughnut tool can define. I think of degrowth as a strategy, or agenda, for reaching a post-growth (or steady-state) economy. Unlike the other terms, degrowth brings in a critical component of justice instead of focusing solely on the environment. Jason Hickel and Kohei Saito both discuss the need to address the dramatic inequality between the Global North and the Global South. Hickel discusses increasing consumption and development in the Global South while decreasing production in the Global North of extractive and destructive production that\u2019s less necessary, such as fast fashion, industrial beef, McMansions, advertising, military armaments, planned obsolescence, food waste, etc.</li> </ul> <p>Circular economy, doughnut economy, post-growth, and degrowth can be complementary. For example, </p>"},{"location":"Other%20topics/Placeholder/","title":"Other topics coming soon","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/","title":"Installing Anaconda in Ubuntu","text":"","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#why-i-chose-to-install-anaconda-in-ubuntu-linux","title":"Why I chose to install Anaconda in Ubuntu Linux","text":"<p>I want to start learning data science and there are a bunch of resources I want to try out (more on those another time). I watched the first two videos of Kevin Markham's Data School video series about pandas. But, in order to get going with pandas I needed to install it. He recommends using the Anaconda distribution of Python which is supposed to have the easiest package installation (numpy, pandas, etc.). From what I've read online, the ease of installation argument is less important for Linux machines where you have admin privileges than for, say, Windows machines where you don't. However, I figured I'd go with the recommended distribution so I could eliminate potential problems and also because I may end up working on a Windows machine later on, so I'd prefer the cross-platform ease of installation of Anaconda. </p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#the-path-variable-problem-that-tripped-me-up","title":"The PATH variable problem that tripped me up","text":"<p>I found installing Anaconda to be really easy. I just went to the installation page, chose the Installing on Linux option and followed the instructions, including verifying the MD5 hash.</p> <p>I followed the FAQ's advice and let the installer add the path to Anaconda to my system's PATH variable. This works fine from Anaconda's perspective, but it changes the default system Python to the version of Python installed with Anaconda. In other words, if you type <code>python</code> at the command line, you'll be running the version of python in the Anaconda directory instead of the default directory (in my case, <code>/usr/bin/python</code>).</p> <p>After way too much time trying to resolve this, the solution I stumbled on turned out to be very simple. Here's the code block that Anaconda added at the end of my <code>~/.bashrc</code> file:</p> <pre><code># added by Anaconda3 5.3.1 installer\n# &gt;&gt;&gt; conda init &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(CONDA_REPORT_ERRORS=false '/home/seth/anaconda3/bin/conda' shell.bash hook 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    \\eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/seth/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/seth/anaconda3/etc/profile.d/conda.sh\"\n        CONDA_CHANGEPS1=false conda activate base\n    else\n        \\export PATH=\"/home/seth/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda init &lt;&lt;&lt;\n</code></pre> <p>The <code>\\export PATH=\"/home/seth/anaconda3/bin:$PATH\"</code> prepends the Anaconda directory to the beginning of your PATH variable's existing values. This is great, except that it means that when the system looks for the Python binaries, it finds them in the Anaconda directory and stops looking there.</p> <p>The solution I came up with (thanks to Piotr Dobrogost's comment on this Stack Overflow question) is to add the default Python directory to the beginning of the PATH variable, prior to the Anaconda directory.</p> <p>To accomplish that, here's the extra two lines of code that I added after the Anaconda code block:</p> <pre><code># added by Anaconda3 5.3.1 installer\n# &gt;&gt;&gt; conda init &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(CONDA_REPORT_ERRORS=false '/home/seth/anaconda3/bin/conda' shell.bash hook 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    \\eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/seth/anaconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/seth/anaconda3/etc/profile.d/conda.sh\"\n        CONDA_CHANGEPS1=false conda activate base\n    else\n        \\export PATH=\"/home/seth/anaconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda init &lt;&lt;&lt;\n\n# add /usr/bin to beginning of PATH so that python, python3, python2 use default system python not Anaconda python \nexport PATH=\"/usr/bin:$PATH\"\n</code></pre> <p>Is there a better, cleaner solution than this? Probably. I notice that this solution ends up putting <code>/usr/bin</code> in my PATH variable twice, which doesn't seem great. But at the moment this seems to be working, so I'm writing it up as a solution to this problem that I ran into.</p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#useful-commands","title":"Useful commands","text":"<ul> <li><code>echo $PATH</code> will print the contents of the PATH variable to the screen. Note that directories are separated by colons.</li> <li><code>source ~/.bashrc</code> will run your <code>.bashrc</code> file and recreate your PATH variable, although note what the Anaconda FAQ has to say about this:</li> </ul> <p>If you have any terminal windows open, close them all then open a new one. You may need to restart your computer for the PATH change to take effect. </p>","tags":["anaconda"]},{"location":"blog/2018/12/04/installing-anaconda-in-ubuntu/#disclaimer","title":"Disclaimer","text":"<p>I'm definitely not a command line or bash expert. Your setup may be different than mine and so these instructions may not apply to you. Of course, if you notice anything that's wrong or could be improved in this post, I'd be glad to hear!</p>","tags":["anaconda"]},{"location":"blog/2018/12/06/getting-started-with-conda/","title":"Getting Started with conda","text":"<p>There are some good resources for getting started with conda on the conda website:</p> <ul> <li>conda cheat sheet</li> <li>Getting started with conda</li> </ul> <p>Here are a few commands that I found useful as a complete newbie to conda:</p> Command What it does <code>conda info</code> displays information about your setup <code>conda info --envs</code> lists your virtual environments <code>conda create --name jupyter --python=3.7</code> create a new virtual environment named <code>jupyter</code> with python version 3.7.x <code>conda create --name jupyter \"python&gt;=3.7\"</code> same as above except python version is greater than or equal to 3.7 (in this case, has the same effect as the previous command); note that you need to use double quotes when using <code>&gt;=</code> <code>conda activate jupyter</code> activate the virtual environment named jupyter; it's possible that your command might be different. You can also try <code>source activate jupyter</code> or <code>activate jupyter</code> in Windows. <code>conda deactivate</code> deactivate the current virtual environment; it's possible that your command might be different. You can also try <code>source deactivate</code> or <code>deactivate</code> in Windows. <code>conda env remove --name jupyter</code> delete the virtual environment named jupyter and everything in it <code>python -V</code> check which version of python you are using <code>which python</code> find the path to the version of python you are using","tags":["anaconda","conda"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/","title":"Getting Started with Jupyter notebook","text":"<p>Here are the steps I took to get started with Jupyter notebook:</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#make-and-activate-a-new-conda-virtual-environment-with-python-3","title":"Make and activate a new conda virtual environment with Python 3","text":"<pre><code>conda create --name jupyter \"python&gt;=3\"\n</code></pre> <p>This creates a new conda virtual environment named <code>jupyter</code> with Python 3 installed. As of December 2018, the latest version is 3.7.1, which is the version that gets installed with the above command. And if the latest version when you're reading this happens to be 3.8.2, then you'll have 3.8.2 in your virtual environment.</p> <pre><code>conda activate jupyter\n</code></pre> <p>This activates the new <code>jupyter</code> virtual environment.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#configure-the-environment-to-allow-python-3-for-the-jupyter-notebooks","title":"Configure the environment to allow Python 3 for the Jupyter notebooks","text":"<p>When I first launched Jupyter, I found that I could only create Jupyter notebooks using Python 2. Python 2 is old and approaching the end of its life, so I wanted to be able to create Jupyter notebooks that use Python 3; plus, I prefer using Python 3's syntax.</p> <p>Thanks to this Stack Overflow answer, I discovered that a couple of more lines of configuration did the trick: </p> <pre><code>conda install notebook ipykernel\nipython kernel install --user\n</code></pre> <p>The first line installs the notebook and ipykernel packages into the active virtual environment (ipython is jupyter's old name).</p> <p>I don't really understand the second line, but I believe it has something to do with registering the ipython kernel to make it available to Jupyter, and the <code>--user</code> flag just means that you're doing it only for your user account.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#make-a-directory-and-optionally-initialize-git","title":"Make a directory and optionally initialize git","text":"<p>I think it's a good idea to have a directory for one's Jupyter notebooks so that they're in one place (unless you want to file them in different directories). It also allows you to then use git on the directory for version control.</p> <p>I made a directory called <code>jupyter</code>, but choose whatever directory name you want.</p> <pre><code>mkdir jupyter\ncd jupyter\n</code></pre> <p>You can also (optionally) initialize git in order to start using version control on the directory:</p> <pre><code>git init\n</code></pre> <p>I'm not going to cover how to use git because that's beyond the scope of this post, but there are some resources here on GitHub's site.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#start-the-jupyter-notebook-server-and-make-a-notebook","title":"Start the Jupyter notebook server and make a notebook","text":"<pre><code>jupyter notebook\n</code></pre> <p>This starts the server locally on your computer and automatically opens up a browser tab pointing to <code>http://localhost:8888/tree</code>, which is the default location and port (8888) for Jupyter notebooks.</p> <p>If you already have created any files or notebooks in the directory then you'll see them there, but if not you'll see an empty directory.</p> <p>In either case, you can make a new Jupyter notebook right in the browser. In the upper-right corner of the page you'll see a dropdown menu called <code>New</code>. If you click on that you should see an option to create a new notebook in whatever language(s) are available in this virtual environment. If you followed the directions above and they (hopefully) worked for you, then you should have an option to create a notebook using Python 3 and possibly Python 2. If you have other languages, you might see options to create notebooks in R, Julia, or perhaps other languages.</p>","tags":["jupyter"]},{"location":"blog/2018/12/07/getting-started-with-jupyter-notebook/#start-using-the-notebook","title":"Start using the notebook","text":"<p>Your new notebook will be blank and look something like this:</p> <p></p> <p>Click on <code>Untitled</code> and give your notebook a name, such as <code>hello_world</code>.</p> <p>Click in the first blank, gray rectangle (called a cell) and enter some Python code, such as this:</p> <pre><code>squares = [num ** 2 for num in range(10)]\nprint(\"Hello world\")\nprint(f\"Here is a list of squares: {squares}\")\n</code></pre> <p>Type SHIFT-ENTER to execute the code. Your notebook should now look something like this:</p> <p></p> <p>Click the first icon which looks like a floppy disk to save your notebook (Jupyter will also autosave your notebook).</p> <p>You can now optionally commit your changes to version control if you'd like.</p> <p>Congratulations, you installed, configured, and created your first Jupyter notebook!</p>","tags":["jupyter"]},{"location":"blog/2018/12/19/data-science-survey-thoughts/","title":"Thoughts on JetBrains' 2018 Data Science Survey","text":"<p>As I\u2019m considering pursuing a career in data science, I found JetBrains 2018 Data Science Survey interesting because it gives me a sense (albeit an imperfect one) of which tools and technologies might be most useful to learn.</p> <p>Here are my takeaways from the survey:</p> <ul> <li>The most popular programming languages regularly used for data analysis are:</li> <li>Python 72%</li> <li>Java 62%</li> <li>R 23%</li> <li>As an aside, Kotlin runs on the Java Virtual Machine, integrates with Hadoop and Spark, and is more concise than Java. It is sponsored by JetBrains, and the survey acknowledges that it likely has some bias, but Kotlin may be an up-and-coming language.</li> <li>Spark is most popular for big data, followed closely by Hadoop. </li> <li>Jupyter notebooks and PyCharm are the most popular IDEs/editors. </li> <li>TensorFlow is the most popular deep learning library. (TensorFlow is lower-level than scikit-learn, according to these Quora answers.) </li> <li>Spreadsheet editors and Tableau are the most popular statistics packages for analyzing and visualizing data. </li> <li>The most popular operating systems are:</li> <li>Windows 62%</li> <li>Linux 44%</li> <li>macOS 37% </li> <li>Computations are performed on:</li> <li>local machines 78%</li> <li>clusters 36%</li> <li>cloud service 32%</li> <li>The most popular cloud services are:</li> <li>Amazon Web Services (AWS) 56%</li> <li>Google Cloud Platform 41%</li> <li>Microsoft Azure 28%</li> <li>The correlation seems to be that the more expertise one\u2019s manager has an data science, the more one tends to agree with this statement: \"My manager gives me realistic assignments that are relevant to my skills and responsibilities, with a clear and specific description of the requirements.\" </li> </ul> <p>It\u2019s nice that I already have experience with Python, Jupyter, PyCharm, spreadsheet editors, Windows and Linux, and AWS.</p> <p>I intend to next learn pandas. </p> <p>After that, my priorities would probably be: </p> <ul> <li>scikit-learn </li> <li>Spark (Hadoop?) </li> <li>TensorFlow </li> <li>Tableau </li> <li>Java </li> </ul>","tags":["meta data science"]},{"location":"blog/2019/01/01/pandas-tips/","title":"pandas Tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#dot-notation-vs-bracket-notation-to-select-a-series","title":"Dot notation vs. bracket notation to select a Series","text":"<p>Each panda Series is essentially a column within a pandas DataFrame.  Whenever a column is added to a DataFrame, the column name (Series) is added  as an attribute on the pandas object. Series  can be accessed through either dot notation or bracket notation:</p> <pre><code># See this code snippet in context at: https://youtu.be/zxqjeyKP2Tk\nimport pandas as pd\nufo = pd.read_table('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/ufo.csv',\n                   sep=',')\nufo_city = ufo['City']  # bracket notation\nufo_city = ufo.City  # dot notation\n</code></pre> <p>You must use bracket notation whenever there is:</p> <ul> <li>a space in the name of a Series</li> <li>a Series name that conflicts with a pandas method</li> </ul> <p>You must also use bracket notation when you want to make a new Series:</p> <pre><code># See this code snippet in context at: https://youtu.be/zxqjeyKP2Tk\nimport pandas as pd\nufo = pd.read_table('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/ufo.csv',\n                   sep=',')\nufo['City_State'] = ufo['City'] + ', ' + ufo['State']\nufo.head()\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#keyboard-tricks-in-jupyter-notebooks","title":"Keyboard tricks in Jupyter notebooks","text":"<ul> <li>You can access all of an object's methods and attributes by typing  <code>&lt;name_of_the_pandas_object&gt;.</code> + <code>TAB-key</code>. (That's the name of the pandas  object, followed by a period and the tab key.)</li> <li>To see the optional or required parameters for a function or a method, when inside of the parentheses, hit <code>SHIFT</code> + <code>TAB</code> one, two, three, or four times.</li> </ul>","tags":["pandas"]},{"location":"blog/2019/01/01/pandas-tips/#a-few-helpful-methods-and-attributes","title":"A few helpful methods and attributes","text":"<ul> <li><code>.describe(include='all')</code> a method that returns statistics about a  DataFrame and produces results like this:</li> </ul> <p> - <code>.shape</code> an attribute that returns a tuple with the number of rows and  columns - <code>.dtypes</code> an attribute that returns the types of each of the Series in  the DataFrame, which produces results like this:</p> <p></p> <p>Note that -- just like in Python -- methods must be followed by <code>()</code> and  attributes must not.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/","title":"Renaming and Removing Columns in pandas DataFrames","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#renaming-columns","title":"Renaming columns","text":"<p>Four methods for renaming columns in a pandas DataFrame:</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#1-rename-specific-columns","title":"1. Rename specific columns","text":"<p><code>foo.rename(columns={}, inplace=True)</code></p> <p>Pass a dict of columns to be renamed. For example:</p> <pre><code>foo.rename(columns={'old_col1': 'new_col1',\n                    'old_col2': 'new_col2'},\n                    inplace=True)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#2-rename-all-the-column-names","title":"2. Rename all the column names","text":"<p>Set <code>.columns</code> to a list of all of the new column names. For example:</p> <pre><code>foo.columns = ['new_col1', 'new_col2']\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#3-rename-the-columns-when-reading-in-a-file","title":"3. Rename the columns when reading in a file","text":"<p>For example:</p> <pre><code>foo_new_names = ['new_col1', 'new_col2']\nfoo = pd.read_csv(data_file.csv,\n                  names=foo_new_names,\n                  header=0)\n</code></pre> <p>Note that in addition to setting the <code>names</code> parameter to a list of the new  column names, you must also set <code>header=0</code> to indicate that you're replacing  the existing column names in the 0th row (if the 0th row is a header row).</p>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#4-replace-existing-spaces-in-column-names-with-underscores","title":"4. Replace existing spaces in column names with underscores:","text":"<pre><code>foo.columns = foo.columns.str.replace(' ', '_')\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/renaming-and-removing-columns/#removing-columns-and-rows","title":"Removing columns (and rows)","text":"<p>Here is the general syntax:</p> <pre><code>foo.drop(col_str_or_list_of_strs,\n         axis=1,  # 0 axis is rows, 1 axis is cols\n         inplace=True)\n</code></pre> <p>to drop a single column:</p> <pre><code>foo.drop('col_name',\n         axis=1,\n         inplace=True)\n</code></pre> <p>to drop multiple columns:</p> <pre><code>foo.drop(['col1', 'col2'],\n         axis=1,\n         inplace=True)\n</code></pre> <p>to drop a single row (data row 0):</p> <pre><code>foo.drop(0,  # int identifying the data row\n         axis=0,\n         inplace=True)\n</code></pre> <p>to drop multiple rows (data rows 1 and 2):</p> <pre><code>foo.drop([1, 2],  # list of ints identifying the data rows\n         axis=0,\n         inplace=True)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/","title":"Sorting and Filtering in pandas","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting","title":"Sorting","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-series-using-dot-notation","title":"Sorting a Series using dot notation","text":"<p>This is a simple as: <pre><code>foo.col_name.sort_values()\n</code></pre></p> <p>To sort in descending order: <pre><code>foo.col_name.sort_values(ascending=False)\n</code></pre></p> <p>Note that the <code>.sort_values()</code> method does not change the underlying sort order . It is a method on a Series that returns a sorted Series.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-dataframe-by-a-single-series","title":"Sorting a DataFrame by a single Series","text":"<p>This is even simpler than the first example!</p> <pre><code>foo.sort_values('col_name')\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#sorting-a-dataframe-by-multiple-series","title":"Sorting a DataFrame by multiple Series","text":"<p>Simply pass a list of the column names you want to sort by:</p> <pre><code>foo.sort_values(['col_name1', 'col_name2'])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering","title":"Filtering","text":"","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering-rows-based-on-a-single-column-criteria","title":"Filtering rows based on a single column criteria","text":"<p>Filtering by a single column criteria is easy in pandas:</p> <pre><code>foo[foo.col_name &gt;= 42]\n</code></pre> <p>To understand how this works, please see my  Jupyter notebook about filtering,  which is based on  Data School's pandas video about filtering.</p>","tags":["pandas"]},{"location":"blog/2019/01/01/sorting-and-filtering-in-pandas/#filtering-rows-based-on-multiple-column-criteria","title":"Filtering rows based on multiple column criteria","text":"<p>Each criterion must be enclosed in parentheses and chained together with  either <code>&amp;</code> (AND) or <code>|</code> (OR). For example:</p> <pre><code>foo[(foo.col_name1 &gt;= 42) &amp; (foo.col_name2 == 'Bar')]\n</code></pre> <p>To use multiple OR criteria for the same column, you can use the <code>isin</code>  method. For example:</p> <pre><code>foo[(foo.col_name1 &gt;= 42) &amp; (foo.col_name2.isin(['Bar', 'Baz', 'Boz']))]\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/","title":"pandas Import and Iteration Tips","text":"<p>This post is based on my takeaways from a Q &amp; A video from Data School.</p>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#import-tips","title":"Import tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#reading-only-certain-columns-during-import-with-usecols-argument","title":"Reading only certain columns during import with <code>usecols</code> argument","text":"<p>The <code>usecols</code> argument can take a list of column names (<code>str</code>s) or column  positions (<code>int</code>s). For example:</p> <pre><code># column names\nfoo = pd.read_csv(data_file,\n                  usecols=['col_name1', 'col_name2'])\n</code></pre> <pre><code># column positions\nfoo = pd.read_csv(data_file,\n                  usecols=[0, 3])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#reading-a-sample-of-rows-during-import-with-nrows-argument","title":"Reading a sample of rows during import with <code>nrows</code> argument","text":"<p>The <code>nrows</code> argument reads the first n rows of a file. For example, to only  import the first three rows from a file:</p> <pre><code>foo = pd.read_csv(data_file,\n                  nrows=3)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#importing-columns-based-on-data-type-using-select_dtypes-include-and-numpy","title":"Importing columns based on data type using <code>.select_dtypes()</code>, <code>include</code>,  and numpy","text":"<p>The <code>select_dtypes()</code> method on a DataFrame allows you to select which  datatypes you want to keep in a DataFrame. The <code>include</code> argument on the  <code>select_dtypes()</code> method indicates which columns you want to include.  In this example, using numpy allows you to specify <code>np.number</code> which includes  number types <code>int64</code>, <code>float64</code>, etc.:</p> <pre><code>import numpy as np\nfoo = pd.read_csv(data_file)\nfoo = foo.select_dtypes(include=[np.number])\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-tips","title":"Iteration tips","text":"","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-through-a-series","title":"Iteration through a Series","text":"<p>This is incredibly easy, and just like iterating over any iterable in Python.  For example:</p> <pre><code>for city in foo.City:  # foo is a pandas DataFrame\n    print(city)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/02/pandas-import-and-iteration-tips/#iteration-through-a-dataframe-with-iterrows","title":"Iteration through a DataFrame with <code>.iterrows()</code>","text":"<p>Using the DataFrame <code>.iterrows()</code> method is similar to <code>enumerate</code> in python:</p> <pre><code>for index, row in foo.iterrows():  # foo is a pandas DataFrame\n    print(index, row.City, row.State)\n</code></pre>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/","title":"pandas string methods","text":"<p>This blog post is based on lesson 12 (\"How do I use string methods in  pandas?\")  from Data School's  pandas video series.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#pandas-has-many-string-methods-available-on-a-series-via-strsome_method","title":"pandas has many string methods available on a Series via <code>.str.some_method()</code>","text":"<p>For example: - <code>df.series_name.str.upper()</code> changes all the strings in the Series called  <code>series_name</code> (in the DataFrame called <code>df</code>) to uppercase - <code>df.series_name.str.title()</code> changes the strings to title case (first  character of each word is capitalized) - String methods on a Series return a Series. In the case of </p> <pre><code>df.series_name.str.contains('bar')\n</code></pre> <p>the <code>.contains()</code> method returns a Series of <code>True</code>s and <code>False</code>s, in which    <code>True</code> is returned if the string in the Series <code>series_name</code> contains <code>bar</code>    and <code>False</code> is returned if the string in the Series <code>series_name</code> does not    contain <code>bar</code>. - You could easily use the <code>True</code>/<code>False</code> Series returned by the <code>.contains()</code>  method above to filter a DataFrame. For example:</p> <pre><code>df[df.series_name.str.contains('bar')]\n</code></pre> <p>will return a new DataFrame filtered to only those rows in which the    <code>series_name</code> Series (aka the column called <code>series_name</code>) contains the    string <code>bar</code>.</p> <p>You can see all of the <code>str</code> methods available in the  pandas API reference.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#string-methods-can-be-chained-together","title":"String methods can be chained together","text":"<p>For example:</p> <pre><code>df.series_name.str.replace('[', '').str.replace(']', '')\n</code></pre> <p>will operate on the Series called <code>series_name</code> in the DataFrame called <code>df</code>. The first <code>.replace()</code> method will replace <code>[</code> with nothing and the second <code>.replace()</code> method will replace <code>]</code> with nothing, allowing you to remove  the brackets from the strings in the Series.</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-string-methods/#many-pandas-string-methods-accept-regular-expressions","title":"Many pandas string methods accept regular expressions","text":"<p>The two chained <code>.replace()</code> methods in the previous example can be replaced  with a singular regex <code>.replace()</code>, like this:</p> <pre><code>df.series_name.str.replace('[\\[\\]]', '')\n</code></pre> <p>Here, the <code>.replace()</code> method is taking the <code>regex</code> string </p> <pre><code>'[\\[\\]]'\n</code></pre> <p>and replacing with nothing. That regular expression can be deconstructed as  follows:</p> <ul> <li>the outer brackets <code>[</code> and <code>]</code> define a character class, meaning that any  of the characters within those character class brackets will be replaced</li> <li>inside the outer brackets is <code>\\[\\]</code>. It represents the two characters  <code>[</code> and <code>]</code> which will be replaced. However, since brackets have a special  meaning in regular expressions, they need to be escaped with backslashes <code>\\</code>.  So the bracket characters to be replaced end up looking like this: </li> </ul> <pre><code>\\[\\]\n</code></pre> <p>You can see working code for all of the above examples in my  Jupyter notebook</p>","tags":["pandas"]},{"location":"blog/2019/01/08/pandas-vs-sql/","title":"pandas vs. SQL","text":"<p>As I'm starting to learn pandas, I noticed that it seems to have the  capabilities of doing everything that SQL can do, and more. So I was curious  about whether I would need SQL in a data science career, or if pandas could  suffice.</p> <p>A quick search turned up this article about PostgreSQL vs. pandas. My takeaway is that, not surprisingly, the situation is more complex than  simply one tool always being better than the other. Instead, SQL is best  suited for certain tasks, and vice versa for pandas.</p> <p>In particular, SQL is faster for typical database tasks, such as joins.  Whereas pandas is better for complex operations like string manipulation or  statistics.</p> <p>While, in theory, you could do practically everything with either tool, it  wouldn't be the most performant solution (pandas and SQL are each faster at  certain tasks, under certain situations), nor the most desirable  solution (e.g. SQL is a lingua franca and compatible with lots of  programming languages, so pandas could be limiting in that regard).</p> <p>The article, titled \"PostgreSQL vs. pandas\u200a\u2014\u200ahow to balance tasks between  server and client side\", offers these ten rules of thumb for evaluating whether to use SQL or pandas  for an analytics task:</p> <ol> <li>If doing a task in SQL can cut the amount of data returned to the client  (e.g. filtering to a smaller subset of data), then the task belongs on the  server.</li> <li>If the amount of data returned to the client remains unchanged or grows  (e.g. adding complex calculated columns; cross-joins, etc.) by doing it in   SQL, the task belongs into client side code.</li> <li>Test different setups on the server and client side to see which is more  efficient. In the worst case, you\u2019ll learn something.</li> <li>Never do in code what the SQL server can do well for you: Data  extraction  (CRUD, joins and set operations) &amp; simple data analysis.</li> <li>If it\u2019s painful or ugly, do it in client-side code: Complex data analysis  belongs into code. Leave formatting or math for the client side. The database  exists mainly to facilitate fast extraction of data.</li> <li>Minimise SQL complexity: Split overly complex, non-performant queries.  Two simpler queries will save the headache of maintaining one mega-query.  Alternatively, split it into a simple query and handle the complexity in  client-side code.</li> <li>Minimise database round trips: Try to do as much as you can in one  operation. Every semicolon is one round trip and adds another I/O operation.</li> <li>Configure your database carefully e.g. for postgres.  Otherwise, you default to sub-optimal algorithms which is expensive.</li> <li>It\u2019s well worth investing time in database schema optimisation.</li> <li>Same goes for setting optimal foreign/sort/distribution keys and  properly normalised tables to maintain the integrity of data.</li> </ol> <p>I think the full article is a worthwhile read.</p>","tags":["pandas"]},{"location":"blog/2019/01/23/next-data-science-steps/","title":"Next Data Science Steps and What I've Been Up To Recently","text":"<p>I haven't blogged in about two weeks. Here's what I've been up to with  data science and programming recently.</p> <ul> <li> <p>I had a small but interesting consulting project with a client I've  previously worked with. It involved a proof of concept to read and write  records via the APIs of a proprietary CRM system. It was an opportunity for  me to work with OAuth (OAuth2, actually) for the first time. I was  pleasantly surprised by how the requests library (which I've used before and liked) has built-in OAuth2 support via the requests-oauthlib package  and this made it much easier to work with OAuth2 than I expected. </p> </li> <li> <p>Here are the details on requests' support for Web Application  Flow (which is the flow I needed to use for this project).</p> </li> <li>And here is an article  from Digital Ocean about the different types of OAuth2 flows which was recommended to me by someone with more OAuth2 experience than I  have.</li> </ul> <p>This was also a chance for me to work directly with the API,    as there isn't a library available that I'm aware of. In contrast, when    I've previously worked with Salesforce's APIs I used the    <code>simple_salesforce</code> Python package, which worked well and which I was    grateful for, but it did introduce a layer of abstraction that led to a    slightly more superficial understanding.</p> <ul> <li> <p>I had another helpful informational interview with a data scientist doing  machine learning. From this, I realized that I want to test whether I'm interested in machine learning, because that will influence my learning path. He recommended Andrew's Ng's Machine Learning Coursera course. I respect this person's recommendation because he has many years of  programming experience, completed a data science masters degree at a  prestigious university, and is working in the industry. Although the course  material is fairly old, he thinks that Andrew explains the material really  well. It's a 55-hour course (free!) and I've recently gotten started on it.</p> </li> <li> <p>Finally, another client I've been assisting with work that's tangentially  related to programming has more directly involved me in programming.  I've been getting my toes wet with Ruby on Rails and it's my first time  working  with a production web app. It's been interesting trying to wrap my brain  around all the parts of a web app (still more to learn!) and to see how  things are set up at a startup where the founders are all accomplished  developers.</p> </li> </ul>","tags":["meta data science"]},{"location":"blog/2019/01/23/web-scraping-embedded-json/","title":"Web scraping a site with embedded JSON","text":"<p>code snippet</p> <pre><code>def scrape_list():\n    \"\"\"Scrape the cleantech list and put companies into a list of dicts.\"\"\"\n    cleantech100_url = 'https://i3connect.com/gct100/the-list'\n    cleantech100_base_url = 'https://i3connect.com'\n    request = requests.get(cleantech100_url)\n    bs = BeautifulSoup(request.content, \"html.parser\")\n    table = bs.table\n    # The HTML table has the headers: COMPANY, GEOGRAPHY, FUNDING, SECTOR,\n    # YEAR FOUNDED\n    header = [\n        'cleantech_url',     # from COMPANY\n        'company_country',   # from GEOGRAPHY\n        'company_funding',   # from FUNDING\n        'company_sector',    # from SECTOR\n        'company_year_founded',  # from YEAR FOUNDED\n        'company_region',    # Column exists but is not displayed in the header.\n        'company_video'      # Column exists but is not displayed in the header.\n    ]\n    companies = []\n    for row in table.tbody.find_all('tr'):\n        company = {}\n        index = 0\n        if 'id' in row.attrs and row.attrs['id'] == 'gct-table-no-results':\n            # Last row of table should be skipped because it's just got this:\n            # &lt;tr id=\"gct-table-no-results\"&gt;\n            #               &lt;td colspan=\"7\"&gt;No results found.&lt;/td&gt;\n            continue\n        for cell in row.find_all('td'):\n            # co_key is the key to use within company dict,\n            # e.g. company[co_key] could point to company['cleantech_url']\n            co_key = header[index]\n            if cell.string is None:\n                # The first and last columns of the HTML table have no text\n                # (cell.string is None).\n                try:\n                    # The first column of the HTML table holds a link to the\n                    # company detail page. This is handled by the try statement,\n                    # as there is an `href` attribute within the &lt;a&gt; tag.\n                    company[co_key] = cleantech100_base_url + cell.a.get('href')\n                except AttributeError:\n                    # The last column of the HTML table holds iframe links to\n                    # videos. Therefore, there is no &lt;a&gt; tag within the cell,\n                    # only a &lt;span&gt; element with a 'data-video-iframe' element.\n                    # This is handled by the except statement.\n                    video_url = cell.span.get('data-video-iframe')\n                    if len(video_url) &gt; 10:\n                        company[co_key] = video_url\n            else:\n                company[co_key] = cell.string\n            index += 1\n        companies.append(company)\n    return companies\n</code></pre>","tags":["meta data science"]},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/data-science/","title":"data science","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/category/data-science/page/2/","title":"data science","text":""}]}